{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5539877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17743fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amex model evaluation metric that is provided in the competition\n",
    "#We won't be using this as its slower than the NP one below\n",
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a108e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faster Amex metric sourced from the following discussion post.\n",
    "#https://www.kaggle.com/competitions/amex-default-prediction/discussion/328020\n",
    "def amex_metric_np(target: np.ndarray, preds: np.ndarray) -> float:\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight * (1 / weight.sum())).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / n_pos\n",
    "\n",
    "    lorentz = (target * (1 / n_pos)).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg))\n",
    "\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c17c5eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This was used to plot % NaN graph.\n",
    "def plot_NaN(training: np.ndarray) -> None:\n",
    "    total = []\n",
    "    for f in training.columns:\n",
    "        total.append(((len(training.loc[training[f] == -127]) / len(training[f])) * 100, f))\n",
    "    sorted_list = sorted(total)\n",
    "    values, cols = zip(*sorted_list)\n",
    "    fig = plt.figure(figsize =(10, 20))\n",
    "    plt.xlabel(\"Percentage makeup\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.title(\"Top 75 features by % makeup of NaN\")\n",
    "    plt.grid()\n",
    "    plt.barh(cols[-75:],values[-75:])\n",
    "    plt.show()\n",
    "    \n",
    "    del total, sorted_list\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bda509d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Kaggle/amex\n"
     ]
    }
   ],
   "source": [
    "#Change to the directory where I have the files on my computer\n",
    "os.chdir(\"/mnt/d/Kaggle/amex\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38e77efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data: (5531451, 160)\n"
     ]
    }
   ],
   "source": [
    "#We will read the data into RAM and do our preprocessing.\n",
    "def get_x_data(filename):\n",
    "    #Read the parquet file into a data frame\n",
    "    df = pd.read_parquet(filename)\n",
    "    #Reducing the customer ID from a 64 byte string to a 8 byte Int64 to reduce the memory footprint\n",
    "    df['customer_ID'] = df['customer_ID'].apply(lambda x: int(x[-16:], 16) ).astype('int64')\n",
    "    #Convert the datetime to a time\n",
    "    df.S_2 = pd.to_datetime(df.S_2)\n",
    "    #Replace NaN values with -127, the lowest you can go in an 8 bit integer. As that lowest common datatype of this parquet dataset.\n",
    "    df = df.fillna(-127)\n",
    "    #We will drop all the categorical features\n",
    "    cat_features = [\"B_30\", \"B_38\", \"D_114\", \"D_116\", \"D_117\", \"D_120\", \"D_126\", \"D_63\", \"D_64\", \"D_66\", \"D_68\"]\n",
    "    #We will drop all features which have > 40% constitution of NaN values of the training set. This is also done on the test set.\n",
    "    to_drop = [\"D_77\", \"S_9\", \"D_56\", \"D_105\", \"B_17\", \"D_50\", \"D_53\", \"D_142\", \"D_42\", \"D_76\", \"D_132\", \"B_29\", \"D_134\", \"B_42\", \"D_73\", \"B_39\", \"D_110\", \"D_88\"]\n",
    "    \n",
    "    #Remove categorical features, features > 40% NaN and the dates\n",
    "    df.drop(to_drop + cat_features + ['S_2'], axis=1, inplace=True)\n",
    "    print('shape of data:', df.shape)\n",
    "    return df\n",
    "\n",
    "df = get_x_data(\"train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "150c9071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We now standardise our data. It is normalised but not standardised.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "index = df['customer_ID']\n",
    "temp_df = pd.DataFrame(StandardScaler().fit_transform(df))\n",
    "temp_df.columns = df.columns\n",
    "temp_df.index = df.index\n",
    "temp_df['customer_ID'] = index\n",
    "\n",
    "#Free memory as memory is the main issue with this project.\n",
    "del df, index\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a6ad537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458913, 477)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We now create our statistical features which we will use to do machine learning upon.\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def aggregate_data(df):\n",
    "    #Aggregate the time series data for each customer into the mean, standard deviation, minimum, max and last values as features\n",
    "    columns = [c for c in list(df.columns) if c not in ['customer_ID', 'S_2']]\n",
    "    data_agg = df.groupby(\"customer_ID\")[columns].agg(['min', 'max', 'last'])\n",
    "    data_agg.columns = ['_'.join(x) for x in data_agg.columns]\n",
    "\n",
    "    #Impute by replacing all NaN's with -1\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=-1)\n",
    "    itrain = pd.DataFrame(imputer.fit_transform(data_agg))\n",
    "    itrain.columns = data_agg.columns\n",
    "    itrain.index = data_agg.index\n",
    "\n",
    "    #Free memory\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    #Impute by replacing all -127's with -1\n",
    "    imputer1 = SimpleImputer(missing_values=-127, strategy='constant', fill_value=-1)\n",
    "    iitrain = pd.DataFrame(imputer1.fit_transform(itrain))\n",
    "    iitrain.columns = itrain.columns\n",
    "    iitrain.index = itrain.index\n",
    "\n",
    "    #Garbage collect\n",
    "    del data_agg, itrain\n",
    "    gc.collect()\n",
    "    \n",
    "    print(iitrain.shape)\n",
    "    \n",
    "    return iitrain\n",
    "\n",
    "#Uncomment to plot the graph of top 75 features consisting of NaN\n",
    "#plot_NaN(temp_df)\n",
    "\n",
    "\n",
    "training = aggregate_data(temp_df)\n",
    "\n",
    "del temp_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad8aaa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now append our labels to our dataset so we only have to move one object around.\n",
    "raw_y = pd.read_csv(\"train_labels.csv\")\n",
    "raw_y['customer_ID'] = raw_y['customer_ID'].apply(lambda x: int(x[-16:], 16) ).astype('int64')\n",
    "raw_y.set_index('customer_ID', inplace=True)\n",
    "\n",
    "training = training.merge(raw_y, left_index=True, right_index=True, how='left')\n",
    "training.target = training.target.astype('int8')\n",
    "\n",
    "del raw_y\n",
    "gc.collect()\n",
    "\n",
    "#We sort our index and reset as sometimes it doesn't reset nicely after appending the Y column\n",
    "training = training.sort_index().reset_index()\n",
    "\n",
    "#Features are all the columns except customer_ID\n",
    "features = training.columns[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "938e43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We begin teaching our models off this data\n",
    "from sklearn.model_selection import StratifiedKFold as KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "\n",
    "#Random Seed for training and repeatability\n",
    "#Number of CV Folds we want to do\n",
    "seed = 22\n",
    "folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9302be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our 10,000 samples which we are using to 'fit' our hyperparameters to.\n",
    "param_eval_train = training.head(10000)\n",
    "x_param_train = param_eval_train.loc[:, features]\n",
    "y_param_train = param_eval_train.loc[:, 'target']\n",
    "\n",
    "print(x_param_train.shape)\n",
    "print(param_eval_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ebd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use ROC-AUC as our scorer, as the AMEX one does not work for whatever reason.\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "amex_scorer = make_scorer(roc_auc_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for SVM optimal 'C' or inverse learning rate.\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "opt = BayesSearchCV(\n",
    "    LinearSVC(max_iter=1000, ),\n",
    "    {\n",
    "        'C': Real(1e-6, 1e+6, prior='log-uniform')\n",
    "    },\n",
    "    cv=Kfolder,\n",
    "    iid=False,\n",
    "    n_iter=32,\n",
    "    n_points=3,\n",
    "    random_state=seed,\n",
    "    scoring = amex_scorer,\n",
    "    optimizer_kwargs={'base_estimator': 'GP'}\n",
    ")\n",
    "\n",
    "_ = opt.fit(x_param_train, y_param_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21150e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print what was found as the optimal one.\n",
    "#Main issue here is that the algorithm never converges, maybe as we are missing too much data when dropped earlier.\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464fab2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Logistic regression hyperparameter search using Bayes theorem.\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "\n",
    "opt_log = BayesSearchCV(\n",
    "    LogisticRegression(penalty='l2', max_iter=1000),\n",
    "    {\n",
    "        'C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "        'class_weight': Categorical(['None', 'balanced'])\n",
    "    },\n",
    "    cv=Kfolder,\n",
    "    iid=False,\n",
    "    n_iter=32,\n",
    "    n_points=3,\n",
    "    random_state=seed,\n",
    "    scoring = amex_scorer,\n",
    "    optimizer_kwargs={'base_estimator': 'GP'}\n",
    ")\n",
    "\n",
    "_ = opt_log.fit(x_param_train, y_param_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt_log.best_params_)\n",
    "print(opt_log.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3524120e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Hyperparameter searching for our LightGBM model.\n",
    "#We are searching for as said, the learning rate, max_depth and L2 regularisation.\n",
    "w_lgbm = lgbm.LGBMRegressor(\n",
    "    num_leaves=32, \n",
    "    max_depth=32,\n",
    "    objective='binary',\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=100,\n",
    "    n_jobs=8,\n",
    "    random_state=seed,\n",
    "    num_iterations = 300\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'learning_rate': Real(1e-6, 1, prior='log-uniform'),\n",
    "    'max_depth': Integer(1, 256),\n",
    "    'reg_lambda': Real(1e-6, 10, prior='log-uniform')\n",
    "}\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "\n",
    "opt_lgbm = BayesSearchCV(\n",
    "    w_lgbm,\n",
    "    params,\n",
    "    cv=Kfolder,\n",
    "    iid=False,\n",
    "    n_iter=32,\n",
    "    n_points=3,\n",
    "    random_state=seed,\n",
    "    scoring = amex_scorer,\n",
    "    optimizer_kwargs={'base_estimator': 'GP'}\n",
    ")\n",
    "\n",
    "_ = opt_lgbm.fit(x_param_train, y_param_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b44921",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt_lgbm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ed8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter search for XGBoost model.\n",
    "#Searching for optimal learning rate (eta), max_depth and L2 regularisation hyperparameter.\n",
    "w_xgb = xgb.XGBRegressor(\n",
    "    objective='binary:logistic',\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'eta': Real(1e-6, 1, prior='log-uniform'),\n",
    "    'max_depth': Integer(1, 256),\n",
    "    'reg_lambda': Real(1e-6, 10, prior='log-uniform')\n",
    "}\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "\n",
    "opt_xgb = BayesSearchCV(\n",
    "    w_xgb,\n",
    "    params,\n",
    "    cv=Kfolder,\n",
    "    iid=False,\n",
    "    n_iter=32,\n",
    "    n_points=3,\n",
    "    random_state=seed,\n",
    "    scoring = amex_scorer,\n",
    "    optimizer_kwargs={'base_estimator': 'GP'}\n",
    ")\n",
    "\n",
    "_ = opt_xgb.fit(x_param_train, y_param_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e2ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78561d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean this Dataframe as its not needed anymore\n",
    "print(\"Buffer\")\n",
    "del y_param_train, x_param_train, param_eval_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8324cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now fit our Logistic regression model using the 'optimal' weights found earlier.\n",
    "#In fact, since it didn't converge earlier we are not using optimal weights hence why we get a worse performance than naive theorem\n",
    "logistic_regressor = LogisticRegression(solver='newton-cg',\n",
    "                                        penalty='l2',\n",
    "                                        C=0.003974000605090588,\n",
    "                                        class_weight='balanced',\n",
    "                                        verbose=20,\n",
    "                                        max_iter=100,\n",
    "                                        n_jobs=8)\n",
    "\n",
    "total_l_acc = []\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(Kfolder.split(training, training.target)):\n",
    "    print(f'Fold: {fold}')\n",
    "    x_train = training.loc[train_index, features]\n",
    "    y_train = training.loc[train_index, 'target']\n",
    "    x_test = training.loc[test_index, features]\n",
    "    y_test = training.loc[test_index, 'target']\n",
    "    logistic_regressor.fit(x_train, y_train)\n",
    "    oof_predict = logistic_regressor.predict(x_test)\n",
    "    acc = amex_metric_np(y_test.values, oof_predict)\n",
    "    total_l_acc.append(acc)\n",
    "    print(f\"Kaggle metric: {acc}\\n\")\n",
    "\n",
    "    del x_train, y_train, x_test, y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the average Kaggle metric obtained and that is our final measure.\n",
    "print(f\"Average Logistic Regression acc = {sum(total_l_acc)/len(total_l_acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb5cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We fit our default Logistic Regression model.\n",
    "logistic_regressor = LogisticRegression()\n",
    "\n",
    "total_ln_acc = []\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(Kfolder.split(training, training.target)):\n",
    "    print(f'Fold: {fold}')\n",
    "    x_train = training.loc[train_index, features]\n",
    "    y_train = training.loc[train_index, 'target']\n",
    "    x_test = training.loc[test_index, features]\n",
    "    y_test = training.loc[test_index, 'target']\n",
    "    logistic_regressor.fit(x_train, y_train)\n",
    "    oof_predict = logistic_regressor.predict(x_test)\n",
    "    acc = amex_metric_np(y_test.values, oof_predict)\n",
    "    total_ln_acc.append(acc)\n",
    "    print(f\"Kaggle metric: {acc}\\n\")\n",
    "\n",
    "    del x_train, y_train, x_test, y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71cb6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average Logistic Regression (naive) acc = {sum(total_ln_acc)/len(total_ln_acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ee3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We fit our suboptimal SVM model due to the fact that the hyperparameter function diverged.\n",
    "svm = LinearSVC(\n",
    "            penalty = 'l2',\n",
    "            C = 0.0002953245610713147,\n",
    "            class_weight = 'balanced',\n",
    "            verbose = 20,\n",
    "            random_state = seed\n",
    "        )\n",
    "\n",
    "total_svm_acc = []\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(Kfolder.split(training, training.target)):\n",
    "    print(f'Fold: {fold}')\n",
    "    x_train = training.loc[train_index, features]\n",
    "    y_train = training.loc[train_index, 'target']\n",
    "    x_test = training.loc[test_index, features]\n",
    "    y_test = training.loc[test_index, 'target']\n",
    "    svm.fit(x_train, y_train)\n",
    "    oof_predict = svm.predict(x_test)\n",
    "    acc = amex_metric_np(y_test.values, oof_predict)\n",
    "    total_svm_acc.append(acc)\n",
    "    print(f\"Kaggle metric: {acc}\\n\")\n",
    "    \n",
    "    del x_train, y_train, x_test, y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea30ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average LinearSVC acc = {sum(total_svm_acc)/len(total_svm_acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our default / unparameterised linear SVM model.\n",
    "svm = LinearSVC()\n",
    "\n",
    "total_svmn_acc = []\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(Kfolder.split(training, training.target)):\n",
    "    print(f'Fold: {fold}')\n",
    "    x_train = training.loc[train_index, features]\n",
    "    y_train = training.loc[train_index, 'target']\n",
    "    x_test = training.loc[test_index, features]\n",
    "    y_test = training.loc[test_index, 'target']\n",
    "    svm.fit(x_train, y_train)\n",
    "    oof_predict = svm.predict(x_test)\n",
    "    acc = amex_metric_np(y_test.values, oof_predict)\n",
    "    total_svmn_acc.append(acc)\n",
    "    print(f\"Kaggle metric: {acc}\\n\")\n",
    "    \n",
    "    del x_train, y_train, x_test, y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b980179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average LinearSVC (naive) acc = {sum(total_svmn_acc)/len(total_svmn_acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab04c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our LightGBM model using the optimised coefficients found earlier. This is heavily overfitted though to the test data.\n",
    "lgbm_model = lgbm.LGBMRegressor(\n",
    "    num_leaves=32, \n",
    "    max_depth=14,\n",
    "    objective='binary',\n",
    "    learning_rate=0.06538186544824388,\n",
    "    n_estimators=100,\n",
    "    n_jobs=8,\n",
    "    random_state=seed,\n",
    "    num_iterations = 300,\n",
    "    reg_lambda = 0.0013722567290885153\n",
    ")\n",
    "\n",
    "#Good hyperparameters to tune: num_leaves, min_data_in_leaf, max_depth, learning rate\n",
    "total_lgbm_acc = []\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(Kfolder.split(training, training.target)):\n",
    "    print(f'Fold: {fold}')\n",
    "    x_train = training.loc[train_index, features]\n",
    "    y_train = training.loc[train_index, 'target']\n",
    "    x_test = training.loc[test_index, features]\n",
    "    y_test = training.loc[test_index, 'target']\n",
    "    lgbm_model.fit(x_train, y_train, \n",
    "                   eval_set=[(x_test, y_test)],\n",
    "                   callbacks=[lgbm.log_evaluation(period=20)]\n",
    "                  )\n",
    "    oof_predict = lgbm_model.predict(x_test)\n",
    "    acc = amex_metric_np(y_test.values, oof_predict)\n",
    "    total_lgbm_acc.append(acc)\n",
    "    print(f\"Kaggle metric: {acc}\\n\")\n",
    "    #Save model in case it crashes so we can load back.\n",
    "    lgbm_model.booster_.save_model(f'LGBM_fold{fold}.lgbm')\n",
    "    \n",
    "    del x_train, y_train, x_test, y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the top 20 important features to the model and its average accuracy.\n",
    "print(f\"Average LGBM acc = {sum(total_lgbm_acc)/len(total_lgbm_acc)}\")\n",
    "lgbm.plot_importance(lgbm_model, max_num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80342173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No parameters LGBM model.\n",
    "lgbmn_model = lgbm.LGBMRegressor(objective='binary')\n",
    "\n",
    "#Good hyperparameters to tune: num_leaves, min_data_in_leaf, max_depth, learning rate\n",
    "total_lgbmn_acc = []\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(Kfolder.split(training, training.target)):\n",
    "    print(f'Fold: {fold}')\n",
    "    x_train = training.loc[train_index, features]\n",
    "    y_train = training.loc[train_index, 'target']\n",
    "    x_test = training.loc[test_index, features]\n",
    "    y_test = training.loc[test_index, 'target']\n",
    "    lgbmn_model.fit(x_train, y_train, \n",
    "                   eval_set=[(x_test, y_test)],\n",
    "                   callbacks=[lgbm.log_evaluation(period=20)]\n",
    "                  )\n",
    "    oof_predict = lgbmn_model.predict(x_test)\n",
    "    acc = amex_metric_np(y_test.values, oof_predict)\n",
    "    total_lgbmn_acc.append(acc)\n",
    "    print(f\"Kaggle metric: {acc}\\n\")\n",
    "    \n",
    "    del x_train, y_train, x_test, y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec2e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average LGBM acc = {sum(total_lgbmn_acc)/len(total_lgbmn_acc)}\")\n",
    "lgbm.plot_importance(lgbmn_model, max_num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90fa957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our 'optimised' XGB model.\n",
    "#One annoying thing is I think the XGB.fit() method is broken. Could not get it to work but xgb.train works just fine.\n",
    "xgb_parms = { \n",
    "    'max_depth':4, \n",
    "    'learning_rate':0.15242435183974648, \n",
    "    'eval_metric':'logloss',\n",
    "    'objective':'binary:logistic',\n",
    "    'tree_method':'hist',\n",
    "    'predictor':'cpu_predictor',\n",
    "    'random_state':seed,\n",
    "    'nthread': 15,\n",
    "    'reg_lambda': 0.07285728814355859\n",
    "}\n",
    "#Most important parameters:\n",
    "# How many subtrees, maximum tree depth, learning rate, the L1 and L2, \n",
    "\n",
    "total_xgb_acc = []\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(Kfolder.split(training, training.target)):\n",
    "    print(f'Fold: {fold}')\n",
    "    x_train = training.loc[train_index, features]\n",
    "    y_train = training.loc[train_index, 'target']\n",
    "    x_test = training.loc[test_index, features]\n",
    "    y_test = training.loc[test_index, 'target']\n",
    "    dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(data=x_test, label=y_test)\n",
    "    xgb_model = xgb.train(xgb_parms, \n",
    "                dtrain=dtrain,\n",
    "                evals=[(dtrain,'train'),(dtest,'test')],\n",
    "                num_boost_round=300,\n",
    "                early_stopping_rounds=25,\n",
    "                verbose_eval=50)\n",
    "    #Save model for future use in case it crashes\n",
    "    xgb_model.save_model(f'XGB_fold{fold}.xgb')\n",
    "    oof_predict = xgb_model.predict(dtest)\n",
    "    acc = amex_metric_np(y_test.values, oof_predict)\n",
    "    total_xgb_acc.append(acc)\n",
    "    print(f\"Kaggle metric: {acc}\\n\")\n",
    "    del dtrain, dtest, x_train, y_train, x_test, y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average XGBoost acc = {sum(total_xgb_acc)/len(total_xgb_acc)}\")\n",
    "xgb.plot_importance(xgb_model, max_num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8e8694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default XGB Model.\n",
    "xgb_parms = { \n",
    "    'eval_metric':'logloss',\n",
    "    'objective':'binary:logistic',\n",
    "    'tree_method':'hist',\n",
    "    'predictor':'cpu_predictor',\n",
    "    'random_state':seed,\n",
    "    'nthread': 15,\n",
    "}\n",
    "#Most important parameters:\n",
    "# How many subtrees, maximum tree depth, learning rate, the L1 and L2, \n",
    "\n",
    "total_xgbn_acc = []\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(Kfolder.split(training, training.target)):\n",
    "    print(f'Fold: {fold}')\n",
    "    x_train = training.loc[train_index, features]\n",
    "    y_train = training.loc[train_index, 'target']\n",
    "    x_test = training.loc[test_index, features]\n",
    "    y_test = training.loc[test_index, 'target']\n",
    "    dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(data=x_test, label=y_test)\n",
    "    xgbn_model = xgb.train(xgb_parms, \n",
    "                dtrain=dtrain,\n",
    "                evals=[(dtrain,'train'),(dtest,'test')],\n",
    "                num_boost_round=300,\n",
    "                early_stopping_rounds=25,\n",
    "                verbose_eval=50) \n",
    "    oof_predict = xgbn_model.predict(dtest)\n",
    "    acc = amex_metric_np(y_test.values, oof_predict)\n",
    "    total_xgbn_acc.append(acc)\n",
    "    print(f\"Kaggle metric: {acc}\\n\")\n",
    "    del dtrain, dtest, x_train, y_train, x_test, y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a567902",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average XGBoost acc = {sum(total_xgbn_acc)/len(total_xgbn_acc)}\")\n",
    "xgb.plot_importance(xgbn_model, max_num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The accuracy on the training data with only the categorical features removed.\n",
    "lbgm_sum = 0.7885413245585315 + 0.7948476986706113 + 0.7859216651728327 + 0.7908956765698341 + 0.7878287049822715\n",
    "xgboost_sum = 0.7905795204886231 + 0.7957968257272143 + 0.7905462609705127 + 0.7921078535024547 + 0.7909747870189094\n",
    "print(\"Lbgm avg Kaggle score on data with only dropped cat features: {:.3f}\".format(lbgm_sum/5))\n",
    "print(\"xgboost avg Kaggle score on data with only dropped cat features: {:.3f}\".format(xgboost_sum/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87115860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A plot of the data with only categories removed vs post processed with most NaN removed and imputed.\n",
    "lgbm_scores = [lbgm_sum/5, sum(total_lgbm_acc)/len(total_lgbm_acc)]\n",
    "xgb_scores = [xgboost_sum/5, sum(total_xgb_acc)/len(total_xgb_acc)]\n",
    "lgbm_y = [\"LGBM\", \"LGBM_modified_data\"]\n",
    "xgb_y = [\"XGB\", \"XGB_modified_data\"]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle(\"Boosted Tree Algorithms | Raw data vs Modified data model acc\")\n",
    "a = ax1.bar(lgbm_y, lgbm_scores)\n",
    "\n",
    "ax1.bar_label(a)\n",
    "b = ax2.bar(xgb_y, xgb_scores)\n",
    "\n",
    "ax2.bar_label(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of optimised vs Default, not Naive\n",
    "old_scores = [0.5399766712400272, 0.5631376385226396, 0.5137486641058764, 0.5407231366405525, 0.7835906474680508, 0.7808382713687521, 0.7853680973318556, 0.7789781451577884]\n",
    "y_vals = [\"Optimised-logistic\", \"Default-logistic\", \"Optimised-SVM\", \"Default-SVM\", \"Optimised-LGBM\", \"Default-LGBM\", \"Optimised-XGB\", \"Default-XGB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff334594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot into a bargraph to compare optimised vs default to see if it made a difference.\n",
    "plt.figure(figsize =(10, 20))\n",
    "a = plt.barh(y_vals, old_scores)\n",
    "plt.xlabel(\"Kaggle Score\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.title(\"Optimised vs Naive Model\")\n",
    "plt.bar_label(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd174829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To run the following test boxes, you need to first run the first 9 boxes, \n",
    "# then you can start from here to do validation on test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83c83ebf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Training not needed anymore as we are moving on to the test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m training\n\u001b[1;32m      3\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training' is not defined"
     ]
    }
   ],
   "source": [
    "#Training not needed anymore as we are moving on to the test data\n",
    "del training\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4e497f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data: (11363762, 160)\n"
     ]
    }
   ],
   "source": [
    "#We are going to split the test data into smaller groups as its much larger than the train data and doesn't fit into RAM nicely.\n",
    "groups = 4\n",
    "test = get_x_data('test.parquet')[['customer_ID']]\n",
    "#We want to get the unique customer ID's and flatten it to a list.\n",
    "customers = test[['customer_ID']].drop_duplicates().sort_index().values.flatten()\n",
    "group_pop = len(customers) // groups #How big are the groups of people we are working with?\n",
    "rows = []\n",
    "for i in range(groups):\n",
    "    if i == groups - 1: \n",
    "        customer_group_pop = customers[i * group_pop:] #If we are on the final group of people, its the left overs so get all from end of last group to end\n",
    "    else: \n",
    "        customer_group_pop = customers[i * group_pop : (i + 1) * group_pop] #Get the group pop of people \n",
    "    s = test.loc[test.customer_ID.isin(customer_group_pop)].shape[0]\n",
    "    #Get Dataframes of our groups' unique ID's\n",
    "    rows.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "228ae6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data: (11363762, 160)\n",
      "(231155, 477)\n",
      "shape of data: (11363762, 160)\n",
      "(231155, 477)\n",
      "shape of data: (11363762, 160)\n",
      "(231155, 477)\n",
      "shape of data: (11363762, 160)\n",
      "(231156, 477)\n"
     ]
    }
   ],
   "source": [
    "skip_rows = 0\n",
    "customers_predicted = 0\n",
    "#Load in our model from Memory as generally running this separate from training / after restarting to clean RAM.\n",
    "test_predict = [] #Our predict values for each individual\n",
    "my_model = lgbm.Booster(model_file=\"LGBM_v3.3.2_fold0.lgbm\")\n",
    "for k in range(groups):\n",
    "    test = get_x_data('test.parquet')\n",
    "    test = test.iloc[skip_rows:skip_rows + rows[k]] #Get the test data for our first group of people\n",
    "    skip_rows += rows[k] #Increment our skip row as we will jump past this next loop\n",
    "    test = aggregate_data(test) #Do our magic to the data\n",
    "    if k == groups - 1: \n",
    "        test = test.loc[customers[customers_predicted:]] #If we are the final group, get from last customer to the end of data\n",
    "    else: \n",
    "        test = test.loc[customers[customers_predicted : customers_predicted + group_pop]] #Get from where we left off to the start of the next group of people\n",
    "    customers_predicted += group_pop #Increment our starting position.\n",
    "    x_test = test[features] #our test data is consists of only the features which we are using.\n",
    "    predict = my_model.predict(x_test)\n",
    "    test_predict.append(predict)\n",
    "\n",
    "    del x_test, test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53f79a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = np.concatenate(test_predict) #We wish to join all our prediction into one list\n",
    "final_to_csv = pd.DataFrame(index=customers,data={'prediction':test_predict}) #Create our pseudo CSV\n",
    "temp = pd.read_csv('sample_submission.csv')[['customer_ID']] #Open our template handin\n",
    "temp['customer_ID_hash'] = temp['customer_ID'].apply(lambda x: int(x[-16:], 16) ).astype('int64') #Hash those customer_ID's with how we've done to space reduce so we can map our predictions\n",
    "temp = temp.set_index('customer_ID_hash') #Set this temporary hash as the index.\n",
    "temp = temp.merge(final_to_csv[['prediction']], left_index=True, right_index=True, how='left') #Merge on the common Customer_IDs\n",
    "temp = temp.reset_index(drop=True)#Reset our index to the original customer ID\n",
    "temp.to_csv(f'submission_lgbm.csv',index=False) #Write to disk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
