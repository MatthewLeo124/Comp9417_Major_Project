{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5539877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17743fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amex model evaluation metric\n",
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a108e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faster Amex metric sourced from the following discussion post.\n",
    "#https://www.kaggle.com/competitions/amex-default-prediction/discussion/328020\n",
    "def amex_metric_np(target: np.ndarray, preds: np.ndarray) -> float:\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight * (1 / weight.sum())).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / n_pos\n",
    "\n",
    "    lorentz = (target * (1 / n_pos)).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg))\n",
    "\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c17c5eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_NaN(training):\n",
    "    total = []\n",
    "    for f in training.columns:\n",
    "        total.append(((len(training.loc[training[f] == -127]) / len(training[f])) * 100, f))\n",
    "    sorted_list = sorted(total)\n",
    "    values, cols = zip(*sorted_list)\n",
    "    fig = plt.figure(figsize =(10, 20))\n",
    "    plt.xlabel(\"Percentage makeup\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.title(\"Top 75 features by % makeup of NaN\")\n",
    "    plt.grid()\n",
    "    plt.barh(cols[-75:],values[-75:])\n",
    "    plt.show()\n",
    "    \n",
    "    del total, sorted_list\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a121bfe5",
   "metadata": {},
   "source": [
    "Plan of how I am going to execute this:\n",
    "- Feautre engineering\n",
    "    - Trying a median, mean and last of the time series data of each customer and using that for our training data\n",
    "    - Removing categorical features\n",
    "    - PCA Feature selection\n",
    "    - Probably use imputation or set NaN values to something maybe 0 and see how that goes\n",
    "    - Use a subset of the features as whole dataset is too big\n",
    "\n",
    "- Model Training:\n",
    "    - Crossvalidation folds (test from 1-5)\n",
    "    - Comparing Logistic regression, SVM, LightBGM, XGBoost, RandomForest etc.\n",
    "    - Then ensemble the models together to see if we can get a better performing one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bda509d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Kaggle/amex\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/mnt/d/Kaggle/amex\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38e77efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data: (5531451, 160)\n"
     ]
    }
   ],
   "source": [
    "def get_x_data(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "    #Reducing the customer ID from a 64 byte string to a 8 byte Int64\n",
    "    df['customer_ID'] = df['customer_ID'].apply(lambda x: int(x[-16:], 16) ).astype('int64')\n",
    "    df.S_2 = pd.to_datetime(df.S_2)\n",
    "    df = df.fillna(-127)\n",
    "    cat_features = [\"B_30\", \"B_38\", \"D_114\", \"D_116\", \"D_117\", \"D_120\", \"D_126\", \"D_63\", \"D_64\", \"D_66\", \"D_68\"]\n",
    "    to_drop = [\"D_77\", \"S_9\", \"D_56\", \"D_105\", \"B_17\", \"D_50\", \"D_53\", \"D_142\", \"D_42\", \"D_76\", \"D_132\", \"B_29\", \"D_134\", \"B_42\", \"D_73\", \"B_39\", \"D_110\", \"D_88\"]\n",
    "    #Remove categorical features, features > 40% NaN and the dates\n",
    "    df.drop(to_drop + cat_features + ['S_2'], axis=1, inplace=True)\n",
    "    print('shape of data:', df.shape)\n",
    "    return df\n",
    "\n",
    "df = get_x_data(\"train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "150c9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "index = df['customer_ID']\n",
    "temp_df = pd.DataFrame(StandardScaler().fit_transform(df))\n",
    "temp_df.columns = df.columns\n",
    "temp_df.index = df.index\n",
    "temp_df['customer_ID'] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a6ad537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458913, 477)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def aggregate_data(df):\n",
    "    #Aggregate the time series data for each customer into the mean, standard deviation, minimum, max and last values as features\n",
    "    columns = [c for c in list(df.columns) if c not in ['customer_ID', 'S_2']]\n",
    "\n",
    "    data_agg = df.groupby(\"customer_ID\")[columns].agg(['min', 'max', 'last'])\n",
    "    data_agg.columns = ['_'.join(x) for x in data_agg.columns]\n",
    "\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=-1)\n",
    "    itrain = pd.DataFrame(imputer.fit_transform(data_agg))\n",
    "    itrain.columns = data_agg.columns\n",
    "    itrain.index = data_agg.index\n",
    "\n",
    "    imputer1 = SimpleImputer(missing_values=-127, strategy='constant', fill_value=-1)\n",
    "    iitrain = pd.DataFrame(imputer1.fit_transform(itrain))\n",
    "    iitrain.columns = itrain.columns\n",
    "    iitrain.index = itrain.index\n",
    "    \n",
    "    print(iitrain.shape)\n",
    "    \n",
    "    return iitrain\n",
    "\n",
    "#Uncomment to plot the graph of top 75 features consisting of NaN\n",
    "#plot_NaN(df)\n",
    "\n",
    "training = aggregate_data(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad8aaa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 477 features!\n"
     ]
    }
   ],
   "source": [
    "raw_y = pd.read_csv(\"train_labels.csv\")\n",
    "raw_y['customer_ID'] = raw_y['customer_ID'].apply(lambda x: int(x[-16:], 16) ).astype('int64')\n",
    "raw_y.set_index('customer_ID', inplace=True)\n",
    "\n",
    "training = training.merge(raw_y, left_index=True, right_index=True, how='left')\n",
    "training.target = training.target.astype('int8')\n",
    "\n",
    "del raw_y\n",
    "gc.collect()\n",
    "\n",
    "training = training.sort_index().reset_index()\n",
    "\n",
    "FEATURES = training.columns[1:-1]\n",
    "\n",
    "print(f'There are {len(FEATURES)} features!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We begin teaching our models off this data\n",
    "from sklearn.model_selection import StratifiedKFold as KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "\n",
    "#Random Seed for training and repeatability\n",
    "#Number of CV Folds we want to do\n",
    "seed = 22\n",
    "folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9302be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_eval_train = training.head(10000)\n",
    "x_param_train = param_eval_train.loc[:, FEATURES]\n",
    "y_param_train = param_eval_train.loc[:, 'target']\n",
    "\n",
    "print(x_param_train.shape)\n",
    "print(param_eval_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ebd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "amex_scorer = make_scorer(roc_auc_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "opt = BayesSearchCV(\n",
    "    LinearSVC(max_iter=1000, ),\n",
    "    {\n",
    "        'C': Real(1e-6, 1e+6, prior='log-uniform')\n",
    "    },\n",
    "    cv=Kfolder,\n",
    "    iid=False,\n",
    "    n_iter=32,\n",
    "    n_points=3,\n",
    "    random_state=seed,\n",
    "    scoring = amex_scorer,\n",
    "    optimizer_kwargs={'base_estimator': 'GP'}\n",
    ")\n",
    "\n",
    "_ = opt.fit(x_param_train, y_param_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21150e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464fab2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "\n",
    "opt_log = BayesSearchCV(\n",
    "    LogisticRegression(penalty='l2', max_iter=1000),\n",
    "    {\n",
    "        'C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "        'class_weight': Categorical(['None', 'balanced'])\n",
    "    },\n",
    "    cv=Kfolder,\n",
    "    iid=False,\n",
    "    n_iter=32,\n",
    "    n_points=3,\n",
    "    random_state=seed,\n",
    "    scoring = amex_scorer,\n",
    "    optimizer_kwargs={'base_estimator': 'GP'}\n",
    ")\n",
    "\n",
    "_ = opt_log.fit(x_param_train, y_param_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt_log.best_params_)\n",
    "print(opt_log.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3524120e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_lgbm = lgbm.LGBMRegressor(\n",
    "    num_leaves=32, \n",
    "    max_depth=32,\n",
    "    objective='binary',\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=100,\n",
    "    n_jobs=8,\n",
    "    random_state=seed,\n",
    "    num_iterations = 300\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'learning_rate': Real(1e-6, 1, prior='log-uniform'),\n",
    "    'max_depth': Integer(1, 256),\n",
    "    'reg_lambda': Real(1e-6, 10, prior='log-uniform')\n",
    "}\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "\n",
    "opt_lgbm = BayesSearchCV(\n",
    "    w_lgbm,\n",
    "    params,\n",
    "    cv=Kfolder,\n",
    "    iid=False,\n",
    "    n_iter=32,\n",
    "    n_points=3,\n",
    "    random_state=seed,\n",
    "    scoring = amex_scorer,\n",
    "    optimizer_kwargs={'base_estimator': 'GP'}\n",
    ")\n",
    "\n",
    "_ = opt_lgbm.fit(x_param_train, y_param_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b44921",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt_lgbm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ed8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_xgb = xgb.XGBRegressor(\n",
    "    objective='binary:logistic',\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'eta': Real(1e-6, 1, prior='log-uniform'),\n",
    "    'max_depth': Integer(1, 256),\n",
    "    'reg_lambda': Real(1e-6, 10, prior='log-uniform')\n",
    "}\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "\n",
    "opt_xgb = BayesSearchCV(\n",
    "    w_xgb,\n",
    "    params,\n",
    "    cv=Kfolder,\n",
    "    iid=False,\n",
    "    n_iter=32,\n",
    "    n_points=3,\n",
    "    random_state=seed,\n",
    "    scoring = amex_scorer,\n",
    "    optimizer_kwargs={'base_estimator': 'GP'}\n",
    ")\n",
    "\n",
    "_ = opt_xgb.fit(x_param_train, y_param_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e2ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78561d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Buffer\")\n",
    "del y_param_train, x_param_train, param_eval_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8324cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regressor = LogisticRegression(solver='newton-cg',\n",
    "                                        penalty='l2',\n",
    "                                        C=0.003974000605090588,\n",
    "                                        class_weight='balanced',\n",
    "                                        verbose=20,\n",
    "                                        max_iter=100,\n",
    "                                        n_jobs=8)\n",
    "\n",
    "total_l_acc = []\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(Kfolder.split(training, training.target)):\n",
    "    print(f'Fold: {fold}')\n",
    "    x_train = training.loc[train_index, FEATURES]\n",
    "    y_train = training.loc[train_index, 'target']\n",
    "    x_test = training.loc[test_index, FEATURES]\n",
    "    y_test = training.loc[test_index, 'target']\n",
    "    logistic_regressor.fit(x_train, y_train)\n",
    "    oof_predict = logistic_regressor.predict(x_test)\n",
    "    acc = amex_metric_np(y_test.values, oof_predict)\n",
    "    total_l_acc.append(acc)\n",
    "    print(f\"Kaggle metric: {acc}\\n\")\n",
    "\n",
    "    del x_train, y_train, x_test, y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average Logistic Regression acc = {sum(total_l_acc)/len(total_l_acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb5cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regressor = LogisticRegression()\n",
    "\n",
    "total_ln_acc = []\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(Kfolder.split(training, training.target)):\n",
    "    print(f'Fold: {fold}')\n",
    "    x_train = training.loc[train_index, FEATURES]\n",
    "    y_train = training.loc[train_index, 'target']\n",
    "    x_test = training.loc[test_index, FEATURES]\n",
    "    y_test = training.loc[test_index, 'target']\n",
    "    logistic_regressor.fit(x_train, y_train)\n",
    "    oof_predict = logistic_regressor.predict(x_test)\n",
    "    acc = amex_metric_np(y_test.values, oof_predict)\n",
    "    total_ln_acc.append(acc)\n",
    "    print(f\"Kaggle metric: {acc}\\n\")\n",
    "\n",
    "    del x_train, y_train, x_test, y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71cb6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average Logistic Regression (naive) acc = {sum(total_ln_acc)/len(total_ln_acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ee3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(\n",
    "            penalty = 'l2',\n",
    "            C = 0.0002953245610713147,\n",
    "            class_weight = 'balanced',\n",
    "            verbose = 20,\n",
    "            random_state = seed\n",
    "        )\n",
    "\n",
    "total_svm_acc = []\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(Kfolder.split(training, training.target)):\n",
    "    print(f'Fold: {fold}')\n",
    "    x_train = training.loc[train_index, FEATURES]\n",
    "    y_train = training.loc[train_index, 'target']\n",
    "    x_test = training.loc[test_index, FEATURES]\n",
    "    y_test = training.loc[test_index, 'target']\n",
    "    svm.fit(x_train, y_train)\n",
    "    oof_predict = svm.predict(x_test)\n",
    "    acc = amex_metric_np(y_test.values, oof_predict)\n",
    "    total_svm_acc.append(acc)\n",
    "    print(f\"Kaggle metric: {acc}\\n\")\n",
    "    \n",
    "    del x_train, y_train, x_test, y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea30ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average LinearSVC acc = {sum(total_svm_acc)/len(total_svm_acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC()\n",
    "\n",
    "total_svmn_acc = []\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(Kfolder.split(training, training.target)):\n",
    "    print(f'Fold: {fold}')\n",
    "    x_train = training.loc[train_index, FEATURES]\n",
    "    y_train = training.loc[train_index, 'target']\n",
    "    x_test = training.loc[test_index, FEATURES]\n",
    "    y_test = training.loc[test_index, 'target']\n",
    "    svm.fit(x_train, y_train)\n",
    "    oof_predict = svm.predict(x_test)\n",
    "    acc = amex_metric_np(y_test.values, oof_predict)\n",
    "    total_svmn_acc.append(acc)\n",
    "    print(f\"Kaggle metric: {acc}\\n\")\n",
    "    \n",
    "    del x_train, y_train, x_test, y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b980179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average LinearSVC (naive) acc = {sum(total_svmn_acc)/len(total_svmn_acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab04c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = lgbm.LGBMRegressor(\n",
    "    num_leaves=32, \n",
    "    max_depth=14,\n",
    "    objective='binary',\n",
    "    learning_rate=0.06538186544824388,\n",
    "    n_estimators=100,\n",
    "    n_jobs=8,\n",
    "    random_state=seed,\n",
    "    num_iterations = 300,\n",
    "    reg_lambda = 0.0013722567290885153\n",
    ")\n",
    "\n",
    "#Good hyperparameters to tune: num_leaves, min_data_in_leaf, max_depth, learning rate\n",
    "total_lgbm_acc = []\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(Kfolder.split(training, training.target)):\n",
    "    print(f'Fold: {fold}')\n",
    "    x_train = training.loc[train_index, FEATURES]\n",
    "    y_train = training.loc[train_index, 'target']\n",
    "    x_test = training.loc[test_index, FEATURES]\n",
    "    y_test = training.loc[test_index, 'target']\n",
    "    lgbm_model.fit(x_train, y_train, \n",
    "                   eval_set=[(x_test, y_test)],\n",
    "                   callbacks=[lgbm.log_evaluation(period=20)]\n",
    "                  )\n",
    "    oof_predict = lgbm_model.predict(x_test)\n",
    "    acc = amex_metric_np(y_test.values, oof_predict)\n",
    "    total_lgbm_acc.append(acc)\n",
    "    print(f\"Kaggle metric: {acc}\\n\")\n",
    "    lgbm_model.booster_.save_model(f'LGBM_fold{fold}.lgbm')\n",
    "    \n",
    "    del x_train, y_train, x_test, y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average LGBM acc = {sum(total_lgbm_acc)/len(total_lgbm_acc)}\")\n",
    "lgbm.plot_importance(lgbm_model, max_num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80342173",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbmn_model = lgbm.LGBMRegressor(objective='binary')\n",
    "\n",
    "#Good hyperparameters to tune: num_leaves, min_data_in_leaf, max_depth, learning rate\n",
    "total_lgbmn_acc = []\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(Kfolder.split(training, training.target)):\n",
    "    print(f'Fold: {fold}')\n",
    "    x_train = training.loc[train_index, FEATURES]\n",
    "    y_train = training.loc[train_index, 'target']\n",
    "    x_test = training.loc[test_index, FEATURES]\n",
    "    y_test = training.loc[test_index, 'target']\n",
    "    lgbmn_model.fit(x_train, y_train, \n",
    "                   eval_set=[(x_test, y_test)],\n",
    "                   callbacks=[lgbm.log_evaluation(period=20)]\n",
    "                  )\n",
    "    oof_predict = lgbmn_model.predict(x_test)\n",
    "    acc = amex_metric_np(y_test.values, oof_predict)\n",
    "    total_lgbmn_acc.append(acc)\n",
    "    print(f\"Kaggle metric: {acc}\\n\")\n",
    "    \n",
    "    del x_train, y_train, x_test, y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec2e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average LGBM acc = {sum(total_lgbmn_acc)/len(total_lgbmn_acc)}\")\n",
    "lgbm.plot_importance(lgbmn_model, max_num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90fa957",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_parms = { \n",
    "    'max_depth':4, \n",
    "    'learning_rate':0.15242435183974648, \n",
    "    'eval_metric':'logloss',\n",
    "    'objective':'binary:logistic',\n",
    "    'tree_method':'hist',\n",
    "    'predictor':'cpu_predictor',\n",
    "    'random_state':seed,\n",
    "    'nthread': 15,\n",
    "    'reg_lambda': 0.07285728814355859\n",
    "}\n",
    "#Most important parameters:\n",
    "# How many subtrees, maximum tree depth, learning rate, the L1 and L2, \n",
    "\n",
    "total_xgb_acc = []\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(Kfolder.split(training, training.target)):\n",
    "    print(f'Fold: {fold}')\n",
    "    x_train = training.loc[train_index, FEATURES]\n",
    "    y_train = training.loc[train_index, 'target']\n",
    "    x_test = training.loc[test_index, FEATURES]\n",
    "    y_test = training.loc[test_index, 'target']\n",
    "    dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(data=x_test, label=y_test)\n",
    "    xgb_model = xgb.train(xgb_parms, \n",
    "                dtrain=dtrain,\n",
    "                evals=[(dtrain,'train'),(dtest,'test')],\n",
    "                num_boost_round=300,\n",
    "                early_stopping_rounds=25,\n",
    "                verbose_eval=50) \n",
    "    xgb_model.save_model(f'XGB_fold{fold}.xgb')\n",
    "    oof_predict = xgb_model.predict(dtest)\n",
    "    acc = amex_metric_np(y_test.values, oof_predict)\n",
    "    total_xgb_acc.append(acc)\n",
    "    print(f\"Kaggle metric: {acc}\\n\")\n",
    "    del dtrain, dtest, x_train, y_train, x_test, y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average XGBoost acc = {sum(total_xgb_acc)/len(total_xgb_acc)}\")\n",
    "xgb.plot_importance(xgb_model, max_num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8e8694",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_parms = { \n",
    "    'eval_metric':'logloss',\n",
    "    'objective':'binary:logistic',\n",
    "    'tree_method':'hist',\n",
    "    'predictor':'cpu_predictor',\n",
    "    'random_state':seed,\n",
    "    'nthread': 15,\n",
    "}\n",
    "#Most important parameters:\n",
    "# How many subtrees, maximum tree depth, learning rate, the L1 and L2, \n",
    "\n",
    "total_xgbn_acc = []\n",
    "\n",
    "Kfolder = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(Kfolder.split(training, training.target)):\n",
    "    print(f'Fold: {fold}')\n",
    "    x_train = training.loc[train_index, FEATURES]\n",
    "    y_train = training.loc[train_index, 'target']\n",
    "    x_test = training.loc[test_index, FEATURES]\n",
    "    y_test = training.loc[test_index, 'target']\n",
    "    dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(data=x_test, label=y_test)\n",
    "    xgbn_model = xgb.train(xgb_parms, \n",
    "                dtrain=dtrain,\n",
    "                evals=[(dtrain,'train'),(dtest,'test')],\n",
    "                num_boost_round=300,\n",
    "                early_stopping_rounds=25,\n",
    "                verbose_eval=50) \n",
    "    oof_predict = xgbn_model.predict(dtest)\n",
    "    acc = amex_metric_np(y_test.values, oof_predict)\n",
    "    total_xgbn_acc.append(acc)\n",
    "    print(f\"Kaggle metric: {acc}\\n\")\n",
    "    del dtrain, dtest, x_train, y_train, x_test, y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a567902",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average XGBoost acc = {sum(total_xgbn_acc)/len(total_xgbn_acc)}\")\n",
    "xgb.plot_importance(xgbn_model, max_num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbgm_sum = 0.7885413245585315 + 0.7948476986706113 + 0.7859216651728327 + 0.7908956765698341 + 0.7878287049822715\n",
    "xgboost_sum = 0.7905795204886231 + 0.7957968257272143 + 0.7905462609705127 + 0.7921078535024547 + 0.7909747870189094\n",
    "print(\"Lbgm avg Kaggle score on data with only dropped cat features: {:.3f}\".format(lbgm_sum/5))\n",
    "print(\"xgboost avg Kaggle score on data with only dropped cat features: {:.3f}\".format(xgboost_sum/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87115860",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_scores = [lbgm_sum/5, sum(total_lgbm_acc)/len(total_lgbm_acc)]\n",
    "xgb_scores = [xgboost_sum/5, sum(total_xgb_acc)/len(total_xgb_acc)]\n",
    "lgbm_y = [\"LGBM\", \"LGBM_modified_data\"]\n",
    "xgb_y = [\"XGB\", \"XGB_modified_data\"]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle(\"Boosted Tree Algorithms | Raw data vs Modified data model acc\")\n",
    "a = ax1.bar(lgbm_y, lgbm_scores)\n",
    "\n",
    "ax1.bar_label(a)\n",
    "b = ax2.bar(xgb_y, xgb_scores)\n",
    "\n",
    "ax2.bar_label(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_scores = [0.5399766712400272, 0.5631376385226396, 0.5137486641058764, 0.5407231366405525, 0.7835906474680508, 0.7808382713687521, 0.7853680973318556, 0.7789781451577884]\n",
    "y_vals = [\"Optimised-logistic\", \"Naive-logistic\", \"Optimised-SVM\", \"Naive-SVM\", \"Optimised-LGBM\", \"Naive-LGBM\", \"Optimised-XGB\", \"Naive-XGB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff334594",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10, 20))\n",
    "a = plt.barh(y_vals, old_scores)\n",
    "plt.xlabel(\"Kaggle Score\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.title(\"Optimised vs Naive Model\")\n",
    "plt.bar_label(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4e497f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading test data...\n",
      "shape of data: (11363762, 160)\n",
      "We will process test data as 4 separate parts.\n",
      "There will be 231155 customers in each part (except the last part).\n",
      "Below are number of rows in each part:\n",
      "[2841209, 2839857, 2842105, 2840591]\n"
     ]
    }
   ],
   "source": [
    "groups = 4\n",
    "test = get_x_data('test.parquet')[['customer_ID']]\n",
    "customers = test[['customer_ID']].drop_duplicates().sort_index().values.flatten()\n",
    "rows, num_cust = get_rows(customers, test[['customer_ID']], chunks=groups)\n",
    "chunk = len(customers) // groups\n",
    "rows = []\n",
    "for k in range(groups):\n",
    "    if k == groups - 1: \n",
    "        cus_chunk = customers[k * chunk:]\n",
    "    else: \n",
    "        cus_chunk = customers[k * chunk : (k+1) * chunk]\n",
    "    s = test.loc[test.customer_ID.isin(cc)].shape[0]\n",
    "    rows.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "228ae6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading test data...\n",
      "shape of data: (11363762, 160)\n",
      "=> Test part 1 has shape (2841209, 160)\n",
      "(231155, 477)\n",
      "\n",
      "Reading test data...\n",
      "shape of data: (11363762, 160)\n",
      "=> Test part 2 has shape (2839857, 160)\n",
      "(231155, 477)\n",
      "\n",
      "Reading test data...\n",
      "shape of data: (11363762, 160)\n",
      "=> Test part 3 has shape (2842105, 160)\n",
      "(231155, 477)\n",
      "\n",
      "Reading test data...\n",
      "shape of data: (11363762, 160)\n",
      "=> Test part 4 has shape (2840591, 160)\n",
      "(231156, 477)\n"
     ]
    }
   ],
   "source": [
    "skip_rows = 0\n",
    "skip_cust = 0\n",
    "test_predict = []\n",
    "for k in range(NUM_PARTS):\n",
    "    test = get_x_data('test.parquet')\n",
    "    test = test.iloc[skip_rows:skip_rows+rows[k]]\n",
    "    skip_rows += rows[k]\n",
    "    test = aggregate_data(test)\n",
    "    if k == groups - 1: \n",
    "        test = test.loc[customers[skip_cust:]]\n",
    "    else: \n",
    "        test = test.loc[customers[skip_cust:skip_cust+num_cust]]\n",
    "    skip_cust += num_cust\n",
    "    x_test = test[FEATURES]\n",
    "    test = test[['S_5_max']]\n",
    "    my_model = lgbm.Booster(model_file=\"LGBM_fold0.lgbm\")\n",
    "    predict = my_model.predict(x_test)\n",
    "    for f in range(1,folds):\n",
    "        my_model = lgbm.Booster(model_file=f\"LGBM_fold{f}.lgbm\")\n",
    "        predict += my_model.predict(x_test)\n",
    "    preds /= folds\n",
    "    test_predict.append(predict)\n",
    "\n",
    "    del X_test, test, my_model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53f79a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file shape is (924621, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000469ba478561f23a92a868bd366de6f6527a684c9a...</td>\n",
       "      <td>0.787976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...</td>\n",
       "      <td>0.116140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...</td>\n",
       "      <td>0.749072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...</td>\n",
       "      <td>0.338775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9...</td>\n",
       "      <td>0.107996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_ID  prediction\n",
       "0  00000469ba478561f23a92a868bd366de6f6527a684c9a...    0.787976\n",
       "1  00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...    0.116140\n",
       "2  0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...    0.749072\n",
       "3  00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...    0.338775\n",
       "4  00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9...    0.107996"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = np.concatenate(test_preds)\n",
    "test = pd.DataFrame(index=customers,data={'prediction':test_preds})\n",
    "sub = pd.read_csv('sample_submission.csv')[['customer_ID']]\n",
    "sub['customer_ID_hash'] = sub['customer_ID'].apply(lambda x: int(x[-16:], 16) ).astype('int64')\n",
    "sub = sub.set_index('customer_ID_hash')\n",
    "sub = sub.merge(test[['prediction']], left_index=True, right_index=True, how='left')\n",
    "sub = sub.reset_index(drop=True)\n",
    "sub.to_csv(f'submission_lgbm.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
